\documentclass[12pt, titlepage]{article}

\usepackage{amsmath, mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../../Comments}
\input{../../Common}

\begin{document}

\title{Library of Lighting Models: System Verification and Validation Plan for 
Family of Lighting Models} 
\author{Sasha Soraine}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
October 17, 2019 & 1.0 & Original Draft.\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables

\listoffigures

\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can simply reference the SRS
  tables, if appropriate}

\newpage

\pagenumbering{arabic}

This document outlines a system validation and verification plan for the 
implementation of a sub-family of lighting models, based on the Commonality 
Analysis for a Family of Lighting Models. First it will cover general 
information about the system, including the particular design qualities the 
system should emphasize and any relevant documentation. Next it will outline 
the verification plans for the commonality analysis/requirements, system 
design, and implementation. It will then outline the software validation plan. 
Finally it will outline a series of representative test cases that are meant to 
test the functional and non-functional requirements, along with a traceability 
matrix mapping the test cases to particular requirements.

\section{General Information}

\subsection{Summary}
This software implements a sub-family of lighting models. The larger family and 
problem analysis is found in \ref{??}. This software aims to take in user 
specifications about the graphical scene (lights, objects, shading model, 
lighting model, and an observer) and render a fully lit and shaded scene. To do 
this is runs calculations using basic optics principles to approximate light 
behaviour in 3D computer graphics.

\subsection{Objectives}
The goal of this testing is to:

\begin{itemize}
	\item Demonstrate adequate usability of the system,
	\item Demonstrate learnability of the system, and
	\item Evaluate the productivity of the system.
\end{itemize}
%
%\wss{State what is intended to be accomplished.  The objective will be around
%  the qualities that are most important for your project.  You might have
%  something like: ``build confidence in the software correctness,''
%  ``demonstrate adequate usability.'' etc.  You won't list all of the 
%qualities,
%  just those that are most important.}

\pagebreak
\subsection{Relevant Documentation}
This section outlines other documentation that is relevant for understanding 
this document.
\begin{table}[h]
	\begin{tabular}{|p{3.5cm}|p{3cm}|p{5cm}|l|}
		\hline
	\textbf{Document Name} & \textbf{Document Type} & \textbf{Document Purpose} 
	& \textbf{Citation} \\
		\hline
		Commonality Analysis of a Family of Lighting Models& Commonality 
		Analysis & Problem domain description, and scoping to a reasonable 
		implementation size through assumptions and requirements. & \\ 
		\hline
	\end{tabular}
\end{table}
%\wss{Reference relevant documentation.  This will definitely include your SRS}

\section{Plan}
This section outlines the verification and validation plans, including any 
techniques or data sets being used in the testing process. It also outlines the 
members of the verification and validations team.
	
\subsection{Verification and Validation Team}
This section lists the members of the verification and validation team. These 
are individuals who contribute to the verification and validation of the system 
and software design. Individuals listed here have specific roles denoting the 
amount of involvement they will be having in the verification and validation 
process. Primary roles are actively working on it; secondary roles view the 
system when major submissions are made; tertiary roles are asked to contribute 
if able, but are under no obligation to participate.

The verification and validation team includes:

\begin{table}[h]
	\begin{tabular}{|l|l|p{9cm}|}
		\hline
		\textbf{Name} & \textbf{Role} & \textbf{Goal} \\
		\hline
		Sasha Soraine & Primary Reviewer& Ensure the verification and 
		validation 
		process runs smoothly.\\
		Peter Michalski & Secondary Reviewer& Ensure the logical consistency of 
		system 
		design and requirements in accordance with feedback role as expert 
		reviewer. \\
		Dr. Spencer Smith & Secondary Reviewer& Ensure reasonable coverage of 
		design 
		considerations and requirements as part of marking these documents. \\
		CAS 741 Students & Tertiary Reviewers& Ensure general consistency in 
		design and 
		requirements coverage in accordance with feedback role as secondary 
		reviewers.\\
		\hline
	\end{tabular}
\end{table}

\pagebreak
\subsection{CA Verification Plan}

We aim to verify the requirements listed in the Commonality Analysis in the 
following ways:

\begin{itemize}
	\item Have expert level users (familiar with graphics programming) do a 
	close read of the commonality analysis to compare it against existing 
	software tools for functionality.
	\item Review and revise requirements based on feedback from Domain Expert 
	and Secondary Reviewer of CA.
	\item Ask Dr. Smith to review the scope to consider whether the 
	implementation scoping and thus listed requirements is inappropriate.
\end{itemize}

\subsection{Design Verification Plan}

We will be using the following methods to test the design:

\begin{itemize}
	\item Rubber duck testing,
	\item Expert review,
	\item
\end{itemize}

\subsection{Implementation Verification Plan}
The purpose of the implementation verification plan is to perform functional 
testing of the components of the system. As such it measures whether the system 
is behaving inline with the requirements documentation, and whether it meets 
its non-functional requirements thresholds.

We will be using the following methods to test the functional implementation:

\begin{itemize}
	\item Boundary value testing,
	\item Endurance testing,
	\item Error handling testing.
\end{itemize}

We will be using the following methods to test the non-functional 
implementation:

\begin{itemize}
	\item Installation testing,
	\item 
\end{itemize}

These are carried out through the test cases listed in this document.

%\wss{You should at least point to the tests listed in this document and the 
%unit
%  testing plan.}
%
%\wss{In this section you would also give any details of any plans for static 
%verification of
%  the implementation.  Potential techniques include code walkthroughs, code
%  inspection, static analyzers, etc.}

\subsection{Software Validation Plan}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\section{System Test Description}
	
\subsection{Tests for Functional Requirements}
The following sections outline test cases for the functional requirements of 
the system.

This section is divided into different testing areas.

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.}

\subsubsection{Input Testing}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Load a Scene}
~\newline
The following test cases all share the same properties:

\begin{itemize}
	\item[] Initial State: No scene loaded.
\end{itemize}

\begin{enumerate}

\item{loadScene-allValid\\}

Control: Automatic
								
Input: \textit{SCENE\_DIR/valid-AllInputs.JSON}
					
Output: \textit{RENDERS\_DIR/render-valid-AllInputs}

Test Case Derivation: Having received a properly formatted and structured JSON 
file containing all inputs for the scene, the system will output a fully 
rendered and lit scene.
					
How test will be performed: System will try to load created test file from 
scene directory.
					
\item{loadScene-validMissingSome\\}

Control: Automatic

Input: \textit{SCENE\_DIR/valid-MissingData.JSON}

Output: Prompt Message: ``File exists, but is missing data. Would you like to 
load the default settings for missing data?", Log message "Error: File exists 
but is missing data."

Test Case Derivation: 

How test will be performed: System will try to load created test file from 
scene directory.

\item{loadScene-fileExistNoData\\}

Control: Automatic
									
Input: \textit{SCENE\_DIR/valid-NoData.JSON}

Output: Prompt Message ``File exists, but is empty. Would you like to load the 
default scene?", Log message "Error: File exists but is empty."

Test Case Derivation: In receiving an empty file, the system should be robust 
enough to acknowledge the specific error and offer to substitute with the 
default scene.

How test will be performed: System will try to load created test file from 
scene directory.

\item{loadScene-invalidInput\\}

Control: Automatic

Input: \textit{SCENE\_DIR/invalid.JSON}

Output: Error Message``File does not contain valid scene data.", Log message 
"Error: Invalid scene data in file: \textit{SCENE\_DIR/invalid.JSON}."

Test Case Derivation: In receiving invalid data, the system should be robust 
enough to acknowledge the specific error and log it in the log file.

How test will be performed: System will try to load created test file from 
scene directory.

\end{enumerate}

\paragraph{Create Default Scene}
~\newline
\begin{enumerate}
	
	\item{createDefault-validMissingData\\}
	
	Control: Automatic

	Initial State: No scene loaded.	System tried to load 
	\textit{SCENE\_DIR/valid-MissingData.JSON} and displayed Prompt Message.

	Input: YES selected at Prompt Message
	
	Output: \textit{RENDERS\_DIR/render-valid-MissingData-defaultsub}
	
	Test Case Derivation: System corrects partially invalid input by 
	substituting missing information with predetermined default values.
	
	How test will be performed: System will fill in empty input with default 
	values.
	
	\item{createDefault-fileExistsNoData\\}
	
	Control: Automatic
	
	Initial State: No scene loaded.	System tried to load 
	\textit{SCENE\_DIR/valid-NoData.JSON} and displayed associated Prompt 
	Message.
	
	Input: YES selected at Prompt Message
	
	Output: \textit{RENDERS\_DIR/render-valid-NoData-defaultsub}
	
	Test Case Derivation: System corrects partially invalid input by 
	substituting missing information with predetermined default values.
	
	How test will be performed: System will fill in empty input with default 
	values.	
	
\end{enumerate}

\subsubsection{Run-Time Tests}
This subset of tests outline scenarios that may happen during the run-time of 
the program. As such it handles changes to the scene and rendering information.

All of these test cases share the following properties:
\begin{itemize}
	\item[] Initial State: A valid scene 
	(\textit{SCENE\_DIR/valid-AllInputs.JSON}) is loaded to the system.
\end{itemize}

\paragraph{Lighting Model Changes}

\begin{enumerate}
	
	\item{lightModel-valid\\}
	
	Control: Automatic
	
	Input: Select different lighting model from dropdown list.
	
	Output: \textit{RENDERS\_DIR/render-valid-AllInputs} with visuals based on 
	new lighting models.
	
	Test Case Derivation: After a scene is loaded, the user can only interact 
	with the system through its GUI. Lighting model changes are determined by 
	dropdown list selection.
	
	How test will be performed: System will recalculate luminous intensity of 
	points on objects in the scene using the new model. New luminous intensity 
	information will be sent through the rendering pipeline to output visuals.
	
\end{enumerate}

\paragraph{Shading Model Changes}

\begin{enumerate}
	
	\item{shadingModel-valid\\}
	
	Control: Automatic
	
	Input: Select different shading model from dropdown list.
	
	Output: \textit{RENDERS\_DIR/render-valid-AllInputs} with visuals based on 
	new shading models.
	
	Test Case Derivation: After a scene is loaded, the user can only interact 
	with the system through its GUI. Shading model changes are determined by 
	dropdown list selection.
	
	How test will be performed: System will recalculate surface normals of 
	points on objects in the scene using the new model. New surface normal
	information will be sent through the rendering pipeline to output visuals.
	
\end{enumerate}

\paragraph{Object Changes}

\begin{enumerate}
%%Object Material Properties
%Valid changes	
	\item{objMaterialPropChange-valid-ks\\}
	
	Control: Automatic
	
	Input: $k_{s} = 0.5$
	
	Output: \textit{RENDERS\_DIR/render-valid-AllInputs} with visuals changed 
	so that specular reflection uses new value.
	
	Test Case Derivation: $I_{s} = k_{s}\cdot i(p,p_{0}) \cdot \max(0, 
	({L_{r}}\bullet V))^\alpha$. Final colouring of any point in a scene is 
	$(I_{a}+I_{d}+I_{s})\cdot LIGHT\_COLOUR$, therefore changes to the $k_{s}$ 
	impact the specular component of the final scene.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	automatically assigns $k_{s}$ the new value. 
	
	\item{objMaterialPropChange-valid-kd\\}
	
	Control: Automatic
	
	Input: $k_{d} = 0.5$
	
	Output: \textit{RENDERS\_DIR/render-valid-AllInputs} with visuals changed 
	so that specular reflection uses new value.
	
	Test Case Derivation: $I_{d} = k_{d}\cdot i(p,p_{0}) \cdot 
	\max(0,(L_{i}\bullet N))$. Final colouring of any point in a scene is 
	$(I_{a}+I_{d}+I_{s})\cdot LIGHT\_COLOUR$, therefore changes to the $k_{d}$ 
	impact the diffuse component of the final scene.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	automatically assigns $k_{d}$ the new value. 	

	\item{objMaterialPropChange-valid-ka\\}
	
	Control: Automatic
	
	Input: $k_{a} = 0.5$
	
	Output: \textit{RENDERS\_DIR/render-valid-AllInputs} with visuals changed 
	so that specular reflection uses new value.
	
	Test Case Derivation: $I_{a} = k_{a}\cdot i(p,p_{0})$. Final colouring of 
	any point in a scene is $(I_{a}+I_{d}+I_{s})\cdot LIGHT\_COLOUR$, therefore 
	changes to the $k_{a}$ impact the ambient component of the final scene.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	automatically assigns $k_{a}$ the new value. 

	\item{objMaterialPropChange-valid-$\alpha$\\}
	
	Control: Automatic
	
	Input: $\alpha = 2$
	
	Output: \textit{RENDERS\_DIR/render-valid-AllInputs} with visuals changed 
	so that specular reflection uses new value.
	
	Test Case Derivation: $I_{s} = k_{s}\cdot i(p,p_{0}) \cdot \max(0, 
	({L_{r}}\bullet V))^\alpha$. Final colouring of any point in a scene is 
	$(I_{a}+I_{d}+I_{s})\cdot LIGHT\_COLOUR$, therefore changes to the $\alpha$ 
	impact the specular component of the final scene.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	automatically assigns $\alpha$ the new value. 

%Invalid
	\item{objMaterialPropChange-invalid-ks\\}
	
	Control: Automatic
	
	Input: $k_{s} = 2$
	
	Output: Error Message ``New value of $k_{s}$ is outside of bounds. Please 
	enter a different value.''. Log message ``Error: tried to assign $k_{s} = 
	2$''. 
	
	Test Case Derivation: $0 \le k_{s} \le 1$; therefore the new assignment is 
	invalid by the constraints.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	attempts to assign $k_{s}$ the new value. Error message is loaded instead. 
	Error is written to log file.
	
	\item{objMaterialPropChange-invalid-kd\\}
	
	Control: Automatic
	
	Input: $k_{d} = 2$
	
	Output: Error Message ``New value of $k_{d}$ is outside of bounds. Please 
	enter a different value.''. Log message ``Error: tried to assign $k_{d} = 
	2$''. 
	
	Test Case Derivation: $0 \le k_{d} \le 1$; therefore the new assignment is 
	invalid by the constraints.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	attempts to assign $k_{d}$ the new value. Error message is loaded instead. 
	Error is written to log file.
	
	\item{objMaterialPropChange-invalid-ka\\}
	
	Control: Automatic
	
	Input: $k_{a} = 2$
	
	Output: Error Message ``New value of $k_{a}$ is outside of bounds. Please 
	enter a different value.''. Log message ``Error: tried to assign $k_{a} = 
	2$''. 
	
	Test Case Derivation: $0 \le k_{a} \le 1$; therefore the new assignment is 
	invalid by the constraints.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	attempts to assign $k_{a}$ the new value. Error message is loaded instead. 
	Error is written to log file.

	\item{objMaterialPropChange-invalid-$\alpha$\\}
	
	Control: Automatic
	
	Input: $\alpha = -2$
	
	Output: Error Message ``New value of $\alpha$ is outside of bounds. Please 
	enter a different value.''. Log message ``Error: tried to assign $\alpha = 
	-2$''. 
	
	Test Case Derivation: $\alpha : \mathbb{Z_{+}}$; therefore the new 
	assignment 
	is invalid by the constraints.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	attempts to assign $\alpha$ the new value. Error message is loaded instead. 
	Error is written to log file.

%Boundary	
	\item{objMaterialPropChange-bound-ks\\}
	
	Control: Automatic
	
	Input: $k_{s} = 1$
	
	Output: \textit{RENDERS\_DIR/render-valid-AllInputs} with visuals changed 
	so that specular reflection uses new value.
	
	Test Case Derivation: $I_{s} = k_{s}\cdot i(p,p_{0}) \cdot \max(0, 
	({L_{r}}\bullet V))^\alpha$. Final colouring of any point in a scene is 
	$(I_{a}+I_{d}+I_{s})\cdot LIGHT\_COLOUR$, therefore changes to the $k_{s}$ 
	impact the specular component of the final scene.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	automatically assigns $k_{s}$ the new value. 
	
	\item{objMaterialPropChange-bound-kd\\}
	
	Control: Automatic
	
	Input: $k_{d} = 1$
	
	Output: \textit{RENDERS\_DIR/render-valid-AllInputs} with visuals changed 
	so that specular reflection uses new value.
	
	Test Case Derivation: $I_{d} = k_{d}\cdot i(p,p_{0}) \cdot 
	\max(0,(L_{i}\bullet N))$. Final colouring of any point in a scene is 
	$(I_{a}+I_{d}+I_{s})\cdot LIGHT\_COLOUR$, therefore changes to the $k_{d}$ 
	impact the diffuse component of the final scene.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	automatically assigns $k_{d}$ the new value. 	
	
	\item{objMaterialPropChange-bound-ka\\}
	
	Control: Automatic
	
	Input: $k_{a} = 1$
	
	Output: \textit{RENDERS\_DIR/render-valid-AllInputs} with visuals changed 
	so that specular reflection uses new value.
	
	Test Case Derivation: $I_{a} = k_{a}\cdot i(p,p_{0})$. Final colouring of 
	any point in a scene is $(I_{a}+I_{d}+I_{s})\cdot LIGHT\_COLOUR$, therefore 
	changes to the $k_{a}$ impact the ambient component of the final scene.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	automatically assigns $k_{a}$ the new value. 
	
	\item{objMaterialPropChange-bound-$\alpha$\\}
	
	Control: Automatic
	
	Input: $\alpha = 0$
	
	Output: \textit{RENDERS\_DIR/render-valid-AllInputs} with visuals changed 
	so that specular reflection uses new value.
	
	Test Case Derivation: $I_{s} = k_{s}\cdot i(p,p_{0}) \cdot \max(0, 
	({L_{r}}\bullet V))^\alpha$. Final colouring of any point in a scene is 
	$(I_{a}+I_{d}+I_{s})\cdot LIGHT\_COLOUR$, therefore changes to the $\alpha$ 
	impact the specular component of the final scene.
	
	How test will be performed: Valid scene is loaded. Testing framework 
	automatically assigns $\alpha$ the new value.
	How test will be performed: 

%%Object Position	
%Valid
	\item{test-id2\\}Change Object Position -valid
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
%Invalid
	How test will be performed: 
	
		\item{test-id2\\}Change Object Position - invalid
		
		Control: Manual versus Automatic
		
		Input: 
		
		Output: \wss{The expected result for the given inputs}
		
		Test Case Derivation: \wss{Justify the expected value given in the 
		Output 
			field}
		
		How test will be performed: 
%Boundary		
	\item{test-id2\\}Change Object Position -boundary
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 
	
	\item{test-id2\\}Change Object Colour -valid
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 

	\item{test-id2\\}Change Object Colour - invalid
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 

	\item{test-id2\\}Change Object Shape -valid
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 
	
	\item{test-id2\\}Change Object Shape - invalid
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 	
		
\end{enumerate}

\paragraph{Light Changes}

\begin{enumerate}
	
	\item{test-id2\\}Change Light Position -valid
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 
	
	\item{test-id2\\}Change Light Position - invalid-out of room
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the 
		Output 
		field}
	
	How test will be performed: 
	
	\item{test-id2\\}Change Light Position -boundary
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 
	
	\item{test-id2\\}Change Light Position - valid-beside object
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the 
		Output 
		field}
	
	How test will be performed: 
	
	\item{test-id2\\}Change Light Position - valid behind object (obj between 
	viewer and light)
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 

	\item{test-id2\\}Change Light Position - invalid-on top of viwere/object
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the 
		Output 
		field}
	
	How test will be performed: 

	\item{test-id2\\}Change Light Colour -valid
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 
	
	\item{test-id2\\}Change Light Colour - invalid
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 
	
	\item{test-id2\\}Change Light Shape -valid
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 
	
	\item{test-id2\\}Change Light Shape -valid-facing away from objects
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 	
	
	\item{test-id2\\}Change Light Shape - invalid
	
	Control: Manual versus Automatic
	
	Input: 
	
	Output: \wss{The expected result for the given inputs}
	
	Test Case Derivation: \wss{Justify the expected value given in the Output 
		field}
	
	How test will be performed: 	
	
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.}

\wss{Tests related to usability could include conducting a usability test and
  survey.}

\subsubsection{Area of Testing1}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: 
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}
				
\bibliographystyle{plainnat}

\bibliography{SRS}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\end{document}